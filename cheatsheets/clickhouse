-- SQL for Clickhouse (db optimized for log search)

-- ## Datetime functions

-- https://clickhouse.com/docs/en/sql-reference/functions/date-time-functions
toStartOfHour(timestamp) -- : round datetime to start of our
toStartOfInterval(timestamp, INTERVAL 7 DAY)
toDateTime(timestamp, 'America/Los_Angeles')
-- use with Grafana variable
date_trunc('${group_by}', toDateTime(timestamp, 'America/Los_Angeles')) as date,

-- In Grafana, you can use those global variables
-- https://grafana.com/grafana/plugins/grafana-clickhouse-datasource/
-- Check also: ./grafana
where timestamp between $__fromTime and $__toTime

select $__timeInterval(columnName)
-- replaces with:
toStartOfInterval(toDateTime(column), INTERVAL 20 second)

-- If you have an `$interval` grafana variable with '30s', '5m', '1d', etc.
SELECT
    toStartOfInterval(timestamp, INTERVAL parseTimeDelta('${interval}') SECOND) as time,
    avg(value)
FROM table
GROUP BY time_bucket

-- Something that can be used by Grafana Time Series
-- Needs:
-- datetime as "time" column
-- --> otherwise you get "long series must be sorted ascending by time to be converted: Could not process SQL results"
-- "value" column can be anything
-- order by time
-- Make sure you select "Time Series" query type
with events as (
SELECT
timestamp,
strings['event_type'] AS event_type,
strings['status'] AS status,
FROM default.table_name
WHERE application = 'app_name' AND timestamp >= subtractHours(now(), 24*7)
)

select
toStartOfHour(timestamp) as time,
category,
count(*) as value

from events

where 1=1
and event_type = 'WHATEVER'
and status != 'STATUS_SUCCESSFUL'

group by 1, 2
order by 1

-- Typical log query
SELECT
    JSONExtractString(rawJSON, 'http', 'headers', 'referer') AS referer_url,
    JSONExtractString(rawJSON, 'http', 'headers', 'origin') AS origin_domain,
    replaceRegexpOne(JSONExtractString(rawJSON, 'http', 'path'), '(.*/)[^/]*$', '\\1') AS request_path_normalized,
    count(*) AS request_count
FROM logs
WHERE true
    and timestamp >= now() - INTERVAL 12 HOUR
GROUP BY
    referer_url,
    origin_domain,
    request_path_normalized
ORDER BY
    request_count DESC
LIMIT 10;

-- argMax
-- https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/argmax
-- Useful to use in groupby
select argMax(user, salary) from salary;
select argMax(trace_id, timestamp) as last_trace_id from logs;
-- argMax with if
select argMax(reason, if(success='true', timestamp, toDateTime64('1970-01-01', 3))) as last_failure_reason from events;

-- success ratio with countIf(...)
select
toStartOfHour(timestamp) as time,
countIf(status = 'STATUS_SUCCESSFUL') / count(*) as value
from events
where event_type = 'WHATEVER'
group by 1
order by 1

-- bar chart
SELECT
    toHour(time) AS h,
    sum(hits) AS t,
    bar(t, 0, max(t) OVER (), 50) AS bar
FROM wikistat
GROUP BY h
ORDER BY h ASC
-- ┌──h─┬─────────t─┬─bar────────────────────────────────────────────────┐
-- │  0 │ 146208847 │ ██████████████████████████████████████▋            │
-- │  1 │ 143713140 │ █████████████████████████████████████▊             |
-- └────┴───────────┴────────────────────────────────────────────────────┘

-- Show errors individually, but start with the stores with the most errors
SELECT
    store_id,
    order_id,
    error_message,
    COUNT(*) OVER (PARTITION BY store_id) AS error_count
FROM
    your_table_name
ORDER BY
    error_count DESC,
    store_id;

-- Group by showing array of grouped values
select
  store_id,
  count(*),
  -- Format string
  arrayMap((x) -> 'https://tools.example.com' || x || '/details', groupArray(order_id))
from
  table_name
group by 1
order by 2 desc;

-- Group by with subtotals
select ...
group by rollup (1, 2);

-- ## Strings
-- https://clickhouse.com/docs/en/sql-reference/functions/string-replace-functions
select replace(haystack, pattern, replacement);
select replaceOne(event_type, 'DELIVERY_METRIC_EVENT_', '')

-- ## Aggregate functions
select any(colname), first_value(colname), last_value(colname);

-- vim: set ft=sql:
